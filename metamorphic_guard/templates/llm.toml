# LLM evaluation configuration
# Optimized for evaluating LLM models with appropriate executors

[task]
name = "your_llm_task_name"
baseline = "path/to/baseline.py"
candidate = "path/to/candidate.py"

[execution]
n = 100  # Lower default for LLM evaluations
seed = 42
timeout_s = 30.0  # Longer timeout for LLM calls
mem_mb = 1024
parallel = 1

[executor]
executor = "openai"  # or "anthropic", "vllm"
executor_config = '''
{
  "api_key": "${OPENAI_API_KEY}",
  "model": "gpt-3.5-turbo",
  "max_tokens": 512,
  "temperature": 0.0
}
'''

[statistics]
alpha = 0.05
min_delta = 0.02
ci_method = "bootstrap"
bootstrap_samples = 1000

[reporting]
report_dir = "reports"
html_report = "reports/llm_report.html"

[budget]
estimate_cost = true
budget_warning = 10.0  # Warn if cost exceeds $10
budget_action = "warn"



